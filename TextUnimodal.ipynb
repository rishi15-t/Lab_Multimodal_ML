{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " TextUnimodal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonArmitage-res/Lab_Multimodal_ML/blob/master/TextUnimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_vJO-RPBu3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFR9QgbfOeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "!pip install --upgrade tables\n",
        "\n",
        "dataset_embeddings = pd.read_hdf('/content/drive/My Drive/dataset/mm_imdb_embeddings.h5', 'embeddings')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q4nIipMCCoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def Train_Test_Split(data , test_data_fraction = 0.2) :\n",
        "    \n",
        "    mlb = MultiLabelBinarizer()\n",
        "    data_genres_one_hot_encoding = mlb.fit_transform(data['genres'])\n",
        "    Label_names = mlb.classes_\n",
        "    data_genres_one_hot_encoding = pd.DataFrame(data_genres_one_hot_encoding, columns = mlb.classes_)\n",
        "    Data_train, Data_test, Labels_train, Labels_test = train_test_split(data, data_genres_one_hot_encoding, test_size = test_data_fraction)\n",
        "    Labels_train = torch.tensor(Labels_train.values)\n",
        "    Labels_test = torch.tensor(Labels_test.values)\n",
        "    \n",
        "    Data_train = Data_train.reset_index(drop=True)\n",
        "    Data_test = Data_test.reset_index(drop=True)\n",
        "\n",
        "    return (Data_train, Data_test, Labels_train, Labels_test, Label_names)\n",
        "    \n",
        "Data_train, Data_test, Labels_train_tensor, Labels_test_tensor, Label_names = Train_Test_Split(dataset_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igEL7o-Rg3JQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For text unimodal classifier\n",
        "import numpy as np\n",
        "\n",
        "Data_train_tensor = torch.tensor(Data_train['bert_embeddings'])\n",
        "Data_test_tensor = torch.tensor(Data_test['bert_embeddings'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HNnZBLx85nK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertModel\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class BertMultiLabelClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_layer_size = 512, hidden_activation = \"tanh\", input_size = 768, num_labels = 23, dropout = 0.1, gen_embeddings = False):\n",
        "\n",
        "        super(BertMultiLabelClassifier, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.input_size = input_size\n",
        "        self.gen_embeddings = gen_embeddings\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.base_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        self.hidden_layer = torch.nn.Linear(self.input_size, self.hidden_layer_size)\n",
        "        \n",
        "        if(hidden_activation == \"tanh\") :\n",
        "          self.hidden_activation = torch.nn.Tanh()\n",
        "        elif(hidden_activation == \"relu\") :\n",
        "          self.hidden_activation = torch.nn.ReLU()\n",
        "        elif(hidden_activation == \"sigmoid\") :\n",
        "          self.hidden_activation = torch.nn.Sigmoid()\n",
        "        elif(hidden_activation == \"leaky_relu\") :\n",
        "          self.hidden_activation = torch.nn.LeakyReLU()\n",
        "        else :\n",
        "          return(\"Invalid hidden_activation parameter value.\")\n",
        "\n",
        "        self.output_layer = torch.nn.Linear(self.hidden_layer_size, self.num_labels)\n",
        "        self.output_activation = torch.nn.Sigmoid()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, indexed_tokens, segment_ids=None, masked_ids=None):\n",
        "        \n",
        "        if(self.gen_embeddings):\n",
        "            pooled_output = self.base_model(indexed_tokens, segment_ids, masked_ids)\n",
        "            embeddings = pooled_output[1]\n",
        "            return embeddings\n",
        "        else :\n",
        "            embeddings = indexed_tokens\n",
        "        \n",
        "        logits1 = self.hidden_layer(embeddings)\n",
        "        activation1 = self.hidden_activation(logits1)\n",
        "\n",
        "        dropped = self.dropout(activation1)\n",
        "\n",
        "        logits2 = self.output_layer(dropped)\n",
        "        if(self.training) :\n",
        "            return logits2\n",
        "        else :\n",
        "            output = self.output_activation(logits2)\n",
        "            return output\n",
        "\n",
        "\n",
        "    def freeze_base_model(self):\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def unfreeze_base_model(self):\n",
        "        for param in self.base_model.named_parameters():\n",
        "            param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdfGdaKL7UQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "from transformers import BertConfig\n",
        "\n",
        "def GetTextEmbeddings_Bert(dataset, batch_size = 32):\n",
        "  \n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  input_ids = TextTransformation_Bert(dataset['plot'])\n",
        "  sampler = SequentialSampler(input_ids)\n",
        "  dataloader = DataLoader(input_ids, sampler=sampler, batch_size=batch_size)\n",
        "  \n",
        "  results = torch.Tensor().to(device)\n",
        "\n",
        "  #config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "  model = BertMultiLabelClassifier(gen_embeddings = True)\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  for num, batch_data in enumerate(dataloader):\n",
        "      print(num) \n",
        "      indexed_tokens, segment_ids , masked_ids = tuple(t for t in batch_data)   \n",
        "      data = indexed_tokens.to(device)\n",
        "      with torch.no_grad():\n",
        "        emdeddings = model(data)\n",
        "      results = torch.cat((results, emdeddings))\n",
        "\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLPzBlKkBjZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from transformers import BertTokenizer\n",
        "from nltk import tokenize\n",
        "#nltk.download('punkt')\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextTransformation_Bert(Dataset) :\n",
        "\n",
        "    def __init__(self, text_list, max_input_length = 512):\n",
        "\n",
        "        self.text_list = text_list\n",
        "        self.indexed_tokens = []\n",
        "        self.segment_ids = []\n",
        "        self.masked_ids = []\n",
        "        self.max_input_length = max_input_length\n",
        "\n",
        "    def GetIndexedTokens(self, text):\n",
        "        \n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        tokenized_text = tokenizer.tokenize(text)\n",
        "        tokenized_text.append(\"[SEP]\")\n",
        "        tokenized_text.insert(0,\"[CLS]\")\n",
        "        self.indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "        \n",
        "\n",
        "    def GetSegmentIds(self) :\n",
        "        \n",
        "        self.segment_ids = [1] * len(self.indexed_tokens)\n",
        "\n",
        " \n",
        "    def GetMaskedIds(self) :\n",
        "        \n",
        "        self.masked_ids = [1] * len(self.indexed_tokens)\n",
        "\n",
        "\n",
        "    def Padding(self) :\n",
        "\n",
        "        if(len(self.indexed_tokens) < self.max_input_length) :\n",
        "           padding = [0]*(self.max_input_length - len(self.indexed_tokens))\n",
        "           self.indexed_tokens += padding\n",
        "           self.segment_ids += padding\n",
        "           self.masked_ids += padding\n",
        "        else :\n",
        "           del self.indexed_tokens[self.max_input_length:]\n",
        "           del self.segment_ids[self.max_input_length:]\n",
        "           del self.masked_ids[self.max_input_length:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        text = self.text_list[idx]\n",
        "\n",
        "        self.GetIndexedTokens(text)\n",
        "        self.GetSegmentIds()\n",
        "        self.GetMaskedIds()\n",
        "        self.Padding()\n",
        "        \n",
        "        self.indexed_tokens = torch.tensor(self.indexed_tokens)\n",
        "        self.segment_ids = torch.tensor(self.segment_ids)\n",
        "        self.masked_ids = torch.tensor(self.masked_ids)\n",
        "        \n",
        "        return self.indexed_tokens, self.segment_ids , self.masked_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqB7MURPE5dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm, trange\n",
        "!pip install transformers\n",
        "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "'''\n",
        "USAGE :\n",
        "\n",
        "1. If text_unimodal = True\n",
        "   Data_train_tensor , Data_test_tensor : Image embeddings as tensors of shape [#train_samples, 4096], [#test_samples, 4096]\n",
        "\n",
        "2. If image_unimodal = True\n",
        "   Data_train_tensor , Data_test_tensor : Text embeddings as tensors of shape [#train_samples, 768], [#test_samples, 768]\n",
        "\n",
        "\n",
        "3. Labels_train_tensor, Labels_test_tensor : one-hot encoded labels as tensors of shape [#train_samples, 23], [#test_samples, 23]\n",
        "\n",
        "'''\n",
        "\n",
        "class Training_Testing():\n",
        "\n",
        "    def __init__(self, Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, hidden_layer_size = 512,\n",
        "                 Label_names = None, freeze_base_model = True, image_unimodal = False, text_unimodal = False,\n",
        "                 hidden_activation = \"tanh\", batch_size = 32, epochs = 10, sigmoid_thresh = 0.2, learning_rate = 2e-5, num_labels = 23, dropout = 0.1):\n",
        "\n",
        "      \n",
        "      #self.dropout = dropout\n",
        "      #self.hidden_layer_size = hidden_layer_size\n",
        "      #self.hidden_activation = hidden_activation\n",
        "\n",
        "      if(text_unimodal) :\n",
        "        #self.config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "        self.model = BertMultiLabelClassifier(hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, dropout = dropout).cuda()\n",
        "\n",
        "      \n",
        "      elif(image_unimodal) :\n",
        "        self.model = VGG16MultiLabelClassifier(hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, dropout = dropout).cuda()\n",
        " \n",
        "\n",
        "      if(freeze_base_model) :\n",
        "        self.model.freeze_base_model()\n",
        "      self.label_names = Label_names\n",
        "      self.num_labels = num_labels\n",
        "      self.batch_size = batch_size\n",
        "      self.learning_rate = learning_rate\n",
        "      self.epochs = epochs\n",
        "      self.sigmoid_thresh = sigmoid_thresh\n",
        "      self.optimizer = self.SetOptimizer()\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.results = pd.DataFrame(0, index=['Recall','Precision','F_Score'], columns=['micro', 'macro', 'weighted', 'samples']).astype(float)\n",
        "      self.epoch_loss_set = []\n",
        "      self.train_dataloader = self.SetTrainDataloader(Data_train_tensor, Labels_train_tensor)\n",
        "      self.test_dataloader = self.SetTestDataloader(Data_test_tensor, Labels_test_tensor)\n",
        "      self.scheduler = self.SetScheduler()\n",
        "\n",
        "\n",
        "    def SetOptimizer(self) :\n",
        "\n",
        "      optimizer = AdamW(self.model.parameters(), self.learning_rate, eps = 1e-6)\n",
        "      return(optimizer)\n",
        "\n",
        "    \n",
        "\n",
        "    def SetScheduler(self) :\n",
        "\n",
        "      scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps = 0, \n",
        "                                                  num_training_steps = self.epochs*len(self.train_dataloader))\n",
        "      return(scheduler) \n",
        "\n",
        "\n",
        "\n",
        "    def Get_Metrics(self, actual, predicted) :\n",
        "\n",
        "      #acc = metrics.accuracy_score(actual, predicted)\n",
        "      #hamming = metrics.hamming_loss(actual, predicted)\n",
        "      #(metrics.roc_auc_score(actual, predicted, average=average)\n",
        "      averages = ('micro', 'macro', 'weighted', 'samples')\n",
        "      for average in averages:\n",
        "          precision, recall, fscore, _ = metrics.precision_recall_fscore_support(actual, predicted, average=average)\n",
        "          self.results[average]['Recall'] += recall\n",
        "          self.results[average]['Precision'] += precision\n",
        "          self.results[average]['F_Score'] += fscore\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def Plot_Training_Epoch_Loss(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.epoch_loss_set, 'b-o')\n",
        "      plt.title(\"Training loss\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.savefig('Training_Epoch_Loss.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def format_time(self, elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "    def SetTrainDataloader(self, Data_train_tensor, Labels_train_tensor) :\n",
        "\n",
        "      train_dataset = TensorDataset(Data_train_tensor, Labels_train_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size = self.batch_size)\n",
        "      return(train_dataloader)\n",
        "\n",
        "\n",
        "    def SetTestDataloader(self, Data_test_tensor, Labels_test_tensor) :\n",
        "      \n",
        "      test_dataset = TensorDataset(Data_test_tensor, Labels_test_tensor)\n",
        "      test_sampler = SequentialSampler(test_dataset)\n",
        "      test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = Data_test_tensor.shape[0])\n",
        "      return(test_dataloader)\n",
        "\n",
        "\n",
        "    def Train(self) :\n",
        "\n",
        "      for _ in trange(self.epochs, desc=\"Epoch\"):\n",
        "        \n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "    \n",
        "        for step_num, batch_data in enumerate(self.train_dataloader):\n",
        "\n",
        "          # Progress update every 30 batches.\n",
        "          if step_num % 30 == 0 and not step_num == 0:\n",
        "            elapsed = self.format_time(time.time() - t0)\n",
        "            print('  Batch : ',step_num, ' , Time elapsed : ',elapsed)\n",
        "\n",
        "          samples, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "          self.optimizer.zero_grad()\n",
        "          logits = self.model(samples.float())\n",
        "          loss_fct = BCEWithLogitsLoss()\n",
        "          batch_loss = loss_fct(logits.view(-1, self.num_labels).float(), labels.view(-1, self.num_labels).float())\n",
        "          batch_loss.backward()\n",
        "          self.optimizer.step()\n",
        "          self.scheduler.step()\n",
        "          epoch_loss += batch_loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss/len(self.train_dataloader)\n",
        "        print(\"\\nTrain loss for epoch: \",avg_epoch_loss)\n",
        "        print(\"\\nTraining epoch took: {:}\".format(self.format_time(time.time() - t0)))\n",
        "        self.epoch_loss_set.append(avg_epoch_loss)\n",
        "\n",
        "      torch.save(self.model.state_dict(), \"/content/drive/My Drive/dataset/model.pt\")\n",
        "      self.Plot_Training_Epoch_Loss()\n",
        "    \n",
        "\n",
        "    def Test(self) :\n",
        "\n",
        "      # Put model in evaluation mode to evaluate loss on the test set\n",
        "      self.model.eval()\n",
        "\n",
        "      for batch_data in self.test_dataloader:\n",
        "  \n",
        "        samples, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "      \n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        # Forward pass, calculate logit predictions\n",
        "        with torch.no_grad():\n",
        "          output = self.model(samples)\n",
        "\n",
        "        threshold = torch.Tensor([self.sigmoid_thresh]).to(self.device)\n",
        "        predictions = (output > threshold).int()\n",
        "\n",
        "        # Move preds and labels to CPU\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "        labels = labels.to('cpu').numpy()\n",
        "      \n",
        "        self.Get_Metrics(labels, predictions)\n",
        "    \n",
        "      self.results = self.results/len(self.test_dataloader)\n",
        "      #print(\"Test data metrics : \\n\")\n",
        "      \n",
        "      return(self.results)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYMGLBsGEK_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test = Training_Testing(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, Label_names=Label_names\n",
        "                              hidden_layer_size=768, hidden_activation = \"relu\", text_unimodal = True, epochs = 200, batch_size= 256, learning_rate=0.001, dropout=0.1, sigmoid_thresh = 0.2)\n",
        "train_test.Train()\n",
        "train_test.Test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVsJ_9hmLe6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "results_dir = \"/content/drive/My Drive/results\"\n",
        "model_dir = results_dir + \"/TextUnimodal\"\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#For testing varying epochs :\n",
        "def Test_epochs(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, \n",
        "                model_dir, current_dir, \n",
        "                hidden_layer_size = 512, hidden_activation = \"tanh\", batch_size= 512, learning_rate=0.01, dropout=0.1, sigmoid_thresh = 0.2,\n",
        "                epochs = [100, 150, 200]) :\n",
        "\n",
        "  \n",
        "  epochs_dir = model_dir + \"/Epochs\"\n",
        "  if(os.path.exists(epochs_dir)) :\n",
        "     shutil.rmtree(epochs_dir)\n",
        "  \n",
        "  os.mkdir(epochs_dir)\n",
        "\n",
        "\n",
        "  params_dict = {'hidden_layer_size' : hidden_layer_size, 'hidden_activation' : hidden_activation, 'batch_size' : batch_size, 'learning_rate' : learning_rate, 'dropout' : dropout, 'sigmoid_thresh' : sigmoid_thresh}\n",
        "  with open(epochs_dir + '/other_params.txt', 'w') as f:\n",
        "    print(params_dict, file=f)\n",
        "\n",
        "  results = pd.DataFrame(index=['Recall','Precision','F_Score']).astype(float)\n",
        "\n",
        "  for i in epochs :\n",
        "\n",
        "      sub_dir = epochs_dir + '/Epochs_' + str(i)\n",
        "      os.mkdir(sub_dir)\n",
        "      os.chdir(sub_dir)\n",
        "\n",
        "      train_test = Training_Testing(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, text_unimodal = True, \n",
        "                                    hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, epochs = i, \n",
        "                                    batch_size = batch_size, learning_rate = learning_rate, dropout = dropout, sigmoid_thresh = sigmoid_thresh)\n",
        "      train_test.Train()\n",
        "      result = train_test.Test()\n",
        "      print('\\n\\n',result,'\\n\\n')\n",
        "      results['Epochs = ' + str(i)] = result['micro']\n",
        "\n",
        "      os.chdir(current_dir)\n",
        "  \n",
        "  results.to_excel(epochs_dir + \"/Micro_metrics.xlsx\", header = True, index = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#For testing varying batch sizes :\n",
        "def Test_batch_size(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, \n",
        "                    model_dir, current_dir, \n",
        "                    hidden_layer_size = 512, hidden_activation = \"tanh\", epochs = 100, learning_rate=0.01, dropout=0.1, sigmoid_thresh = 0.2,\n",
        "                    batch_size = [64, 128, 256, 512]) :\n",
        "\n",
        "  \n",
        "  batch_dir = model_dir + \"/BatchSizes\"\n",
        "  if(os.path.exists(batch_dir)) :\n",
        "     shutil.rmtree(batch_dir)\n",
        "  \n",
        "  os.mkdir(batch_dir)\n",
        "\n",
        "  params_dict = {'hidden_layer_size' : hidden_layer_size, 'hidden_activation' : hidden_activation, 'epochs' : epochs, 'learning_rate' : learning_rate, 'dropout' : dropout, 'sigmoid_thresh' : sigmoid_thresh}\n",
        "  with open(batch_dir + '/other_params.txt', 'w') as f:\n",
        "    print(params_dict, file=f)\n",
        "\n",
        "  results = pd.DataFrame(index=['Recall','Precision','F_Score']).astype(float)\n",
        "\n",
        "  for i in batch_size :\n",
        "\n",
        "      sub_dir = batch_dir + '/BatchSize_' + str(i)\n",
        "      os.mkdir(sub_dir)\n",
        "      os.chdir(sub_dir)\n",
        "\n",
        "      train_test = Training_Testing(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, text_unimodal = True, \n",
        "                                    hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, epochs = epochs, \n",
        "                                    batch_size = i, learning_rate = learning_rate, dropout = dropout, sigmoid_thresh = sigmoid_thresh)\n",
        "      train_test.Train()\n",
        "      result = train_test.Test()\n",
        "      print('\\n\\n',result,'\\n\\n')\n",
        "      results['Batch Size = ' + str(i)] = result['micro']\n",
        "\n",
        "      os.chdir(current_dir)\n",
        "  \n",
        "  results.to_excel(batch_dir + \"/Micro_metrics.xlsx\", header = True, index = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#For testing varying dropout rates :\n",
        "def Test_dropout_rate(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, \n",
        "                       model_dir, current_dir, \n",
        "                       hidden_layer_size = 512, hidden_activation = \"tanh\", epochs = 100, batch_size = 512, learning_rate = 0.01, sigmoid_thresh = 0.2,\n",
        "                       dropout = [0.1 , 0.2, 0.5]) :\n",
        "\n",
        "  \n",
        "  dropout_rate_dir = model_dir + \"/DropoutRate\"\n",
        "  if(os.path.exists(dropout_rate_dir)) :\n",
        "     shutil.rmtree(dropout_rate_dir)\n",
        "  \n",
        "  os.mkdir(dropout_rate_dir)\n",
        "\n",
        "  params_dict = {'hidden_layer_size' : hidden_layer_size, 'hidden_activation' : hidden_activation, 'epochs' : epochs, 'batch_size' : batch_size, 'learning_rate' : learning_rate, 'sigmoid_thresh' : sigmoid_thresh}\n",
        "  with open(dropout_rate_dir + '/other_params.txt', 'w') as f:\n",
        "    print(params_dict, file=f)\n",
        "\n",
        "  results = pd.DataFrame(index=['Recall','Precision','F_Score']).astype(float)\n",
        "\n",
        "  for i in dropout :\n",
        "\n",
        "      sub_dir = dropout_rate_dir + '/DropoutRate_' + str(i)\n",
        "      os.mkdir(sub_dir)\n",
        "      os.chdir(sub_dir)\n",
        "\n",
        "      train_test = Training_Testing(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, text_unimodal = True, \n",
        "                                    hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, epochs = epochs, \n",
        "                                    batch_size = batch_size, learning_rate = learning_rate, dropout = i, sigmoid_thresh = sigmoid_thresh)\n",
        "      train_test.Train()\n",
        "      result = train_test.Test()\n",
        "      print('\\n\\n',result,'\\n\\n')\n",
        "      results['Dropout Rate = ' + str(i)] = result['micro']\n",
        "\n",
        "      os.chdir(current_dir)\n",
        "  \n",
        "  results.to_excel(dropout_rate_dir + \"/Micro_metrics.xlsx\", header = True, index = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#For testing varying learning rates :\n",
        "def Test_learning_rate(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, \n",
        "                       model_dir, current_dir, \n",
        "                       hidden_layer_size = 512, hidden_activation = \"tanh\", epochs = 100, batch_size = 512, dropout=0.1, sigmoid_thresh = 0.2,\n",
        "                       learning_rate = [0.1 , 0.05, 0.01, 0.001, 0.0001, 0.00001]) :\n",
        "\n",
        "  \n",
        "  learning_rate_dir = model_dir + \"/LearningRate\"\n",
        "  if(os.path.exists(learning_rate_dir)) :\n",
        "     shutil.rmtree(learning_rate_dir)\n",
        "  \n",
        "  os.mkdir(learning_rate_dir)\n",
        "\n",
        "  params_dict = {'hidden_layer_size' : hidden_layer_size, 'hidden_activation' : hidden_activation, 'epochs' : epochs, 'batch_size' : batch_size, 'dropout' : dropout, 'sigmoid_thresh' : sigmoid_thresh}\n",
        "  with open(learning_rate_dir + '/other_params.txt', 'w') as f:\n",
        "    print(params_dict, file=f)\n",
        "\n",
        "  results = pd.DataFrame(index=['Recall','Precision','F_Score']).astype(float)\n",
        "\n",
        "  for i in learning_rate :\n",
        "\n",
        "      sub_dir = learning_rate_dir + '/LearningRate_' + str(i)\n",
        "      os.mkdir(sub_dir)\n",
        "      os.chdir(sub_dir)\n",
        "\n",
        "      train_test = Training_Testing(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, text_unimodal = True, \n",
        "                                    hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, epochs = epochs, \n",
        "                                    batch_size = batch_size, learning_rate = i, dropout = dropout, sigmoid_thresh = sigmoid_thresh)\n",
        "      train_test.Train()\n",
        "      result = train_test.Test()\n",
        "      print('\\n\\n',result,'\\n\\n')\n",
        "      results['Learning Rate = ' + str(i)] = result['micro']\n",
        "\n",
        "      os.chdir(current_dir)\n",
        "  \n",
        "  results.to_excel(learning_rate_dir + \"/Micro_metrics.xlsx\", header = True, index = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#For testing varying activation functions :\n",
        "def Test_activation_fn(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, \n",
        "                       model_dir, current_dir, \n",
        "                       hidden_layer_size = 512, learning_rate = 0.01, epochs = 100, batch_size = 512, dropout=0.1, sigmoid_thresh = 0.2,\n",
        "                       hidden_activation = [\"tanh\", \"relu\", \"sigmoid\", \"leaky_relu\"]) :\n",
        "\n",
        "  \n",
        "  activation_fn_dir = model_dir + \"/ActivationFunc\"\n",
        "  if(os.path.exists(activation_fn_dir)) :\n",
        "     shutil.rmtree(activation_fn_dir)\n",
        "  \n",
        "  os.mkdir(activation_fn_dir)\n",
        "\n",
        "  params_dict = {'hidden_layer_size' : hidden_layer_size, 'learning_rate' : learning_rate, 'epochs' : epochs, 'batch_size' : batch_size, 'dropout' : dropout, 'sigmoid_thresh' : sigmoid_thresh}\n",
        "  with open(activation_fn_dir + '/other_params.txt', 'w') as f:\n",
        "    print(params_dict, file=f)\n",
        "\n",
        "  results = pd.DataFrame(index=['Recall','Precision','F_Score']).astype(float)\n",
        "\n",
        "  for i in hidden_activation :\n",
        "\n",
        "      sub_dir = activation_fn_dir + '/ActivationFunc_' + i\n",
        "      os.mkdir(sub_dir)\n",
        "      os.chdir(sub_dir)\n",
        "\n",
        "      train_test = Training_Testing(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, text_unimodal = True, \n",
        "                                    hidden_layer_size = hidden_layer_size, hidden_activation = i, epochs = epochs, \n",
        "                                    batch_size = batch_size, learning_rate = learning_rate, dropout = dropout, sigmoid_thresh = sigmoid_thresh)\n",
        "      train_test.Train()\n",
        "      result = train_test.Test()\n",
        "      print('\\n\\n',result,'\\n\\n')\n",
        "      results['Activation Function = ' + i] = result['micro']\n",
        "\n",
        "      os.chdir(current_dir)\n",
        "  \n",
        "  results.to_excel(activation_fn_dir + \"/Micro_metrics.xlsx\", header = True, index = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#For testing varying #hidden neurons :\n",
        "def Test_hidden_neurons(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, \n",
        "                       model_dir, current_dir, \n",
        "                       hidden_activation = \"tanh\" , learning_rate = 0.01, epochs = 100, batch_size = 512, dropout = 0.1, sigmoid_thresh = 0.2,\n",
        "                       hidden_layer_size = [512, 256]) :\n",
        "\n",
        "  \n",
        "  hidden_neurons_dir = model_dir + \"/HiddenNeurons\"\n",
        "  if(os.path.exists(hidden_neurons_dir)) :\n",
        "     shutil.rmtree(hidden_neurons_dir)\n",
        "  \n",
        "  os.mkdir(hidden_neurons_dir)\n",
        "\n",
        "  params_dict = {'hidden_activation' : hidden_activation, 'learning_rate' : learning_rate, 'epochs' : epochs, 'batch_size' : batch_size, 'dropout' : dropout, 'sigmoid_thresh' : sigmoid_thresh}\n",
        "  with open(hidden_neurons_dir + '/other_params.txt', 'w') as f:\n",
        "    print(params_dict, file=f)\n",
        "\n",
        "  results = pd.DataFrame(index=['Recall','Precision','F_Score']).astype(float)\n",
        "\n",
        "  for i in hidden_layer_size :\n",
        "\n",
        "      sub_dir = hidden_neurons_dir + '/HiddenNeurons_' + str(i)\n",
        "      os.mkdir(sub_dir)\n",
        "      os.chdir(sub_dir)\n",
        "\n",
        "      train_test = Training_Testing(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, text_unimodal = True, \n",
        "                                    hidden_layer_size = i, hidden_activation = hidden_activation, epochs = epochs, \n",
        "                                    batch_size = batch_size, learning_rate = learning_rate, dropout = dropout, sigmoid_thresh = sigmoid_thresh)\n",
        "      train_test.Train()\n",
        "      result = train_test.Test()\n",
        "      print('\\n\\n',result,'\\n\\n')\n",
        "      results['#Hidden Neurons = ' + str(i)] = result['micro']\n",
        "\n",
        "      os.chdir(current_dir)\n",
        "  \n",
        "  results.to_excel(hidden_neurons_dir + \"/Micro_metrics.xlsx\", header = True, index = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#For testing varying sigmoid threshold :\n",
        "def Test_sigmoid_threshold(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, \n",
        "                       model_dir, current_dir, \n",
        "                       hidden_activation = \"tanh\" , learning_rate = 0.01, epochs = 100, batch_size = 512, dropout = 0.1, hidden_layer_size = 512,\n",
        "                       sigmoid_thresh = [0.15, 0.18, 0.2, 0.23, 0.25]) :\n",
        "\n",
        "  \n",
        "  sigmoid_threshold_dir = model_dir + \"/SigmoidThreshold\"\n",
        "  if(os.path.exists(sigmoid_threshold_dir)) :\n",
        "     shutil.rmtree(sigmoid_threshold_dir)\n",
        "  \n",
        "  os.mkdir(sigmoid_threshold_dir)\n",
        "\n",
        "  params_dict = {'hidden_activation' : hidden_activation, 'learning_rate' : learning_rate, 'epochs' : epochs, 'batch_size' : batch_size, 'dropout' : dropout, 'sigmoid_thresh' : sigmoid_thresh}\n",
        "  with open(sigmoid_threshold_dir + '/other_params.txt', 'w') as f:\n",
        "    print(params_dict, file=f)\n",
        "\n",
        "  results = pd.DataFrame(index=['Recall','Precision','F_Score']).astype(float)\n",
        "\n",
        "  for i in sigmoid_thresh :\n",
        "\n",
        "      sub_dir = sigmoid_threshold_dir + '/SigmoidThreshold_' + str(i)\n",
        "      os.mkdir(sub_dir)\n",
        "      os.chdir(sub_dir)\n",
        "\n",
        "      train_test = Training_Testing(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, text_unimodal = True, \n",
        "                                    hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, epochs = epochs, \n",
        "                                    batch_size = batch_size, learning_rate = learning_rate, dropout = dropout, sigmoid_thresh = i)\n",
        "      train_test.Train()\n",
        "      result = train_test.Test()\n",
        "      print('\\n\\n',result,'\\n\\n')\n",
        "      results['Sigmoid Threshold = ' + str(i)] = result['micro']\n",
        "\n",
        "      os.chdir(current_dir)\n",
        "  \n",
        "  results.to_excel(sigmoid_threshold_dir + \"/Micro_metrics.xlsx\", header = True, index = True)\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXhOKfq1QKPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Test_epochs(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, model_dir, current_dir, \n",
        "            hidden_layer_size = 768, hidden_activation = \"relu\", batch_size= 256, learning_rate=0.001, dropout=0.1, sigmoid_thresh = 0.2, epochs = [100, 150, 200])\n",
        "\n",
        "Test_batch_size(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, model_dir, current_dir, \n",
        "                hidden_layer_size = 768, hidden_activation = \"relu\", learning_rate=0.001, dropout=0.1, epochs = 100, sigmoid_thresh = 0.2, batch_size = [256, 512, 1024])\n",
        "\n",
        "Test_learning_rate(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, model_dir, current_dir, \n",
        "                   hidden_layer_size = 768, hidden_activation = \"relu\", batch_size= 256, dropout=0.1, epochs = 100, sigmoid_thresh = 0.2, learning_rate = [0.1 , 0.05, 0.01, 0.001, 0.0001, 0.00001])\n",
        "\n",
        "Test_dropout_rate(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, model_dir, current_dir, \n",
        "                  hidden_layer_size = 768, hidden_activation = \"relu\", batch_size= 256, learning_rate=0.001, epochs = 100, sigmoid_thresh = 0.2, dropout = [0.1, 0.5])\n",
        "\n",
        "Test_activation_fn(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, model_dir, current_dir, \n",
        "                   hidden_layer_size = 768, batch_size= 256, learning_rate=0.001, epochs = 100, dropout = 0.1, sigmoid_thresh = 0.2, hidden_activation = [\"tanh\", \"relu\", \"sigmoid\", \"leaky_relu\"])\n",
        "\n",
        "Test_hidden_neurons(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, model_dir, current_dir, \n",
        "                    hidden_activation = \"relu\", batch_size= 256, learning_rate=0.001, epochs = 100, dropout = 0.1, sigmoid_thresh = 0.2, hidden_layer_size = [256 , 535, 768, 1000])\n",
        "\n",
        "Test_sigmoid_threshold(Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, model_dir, current_dir, \n",
        "                    hidden_activation = \"relu\", batch_size= 256, learning_rate=0.001, epochs = 100, dropout = 0.1, hidden_layer_size = 768, sigmoid_thresh = [0.15, 0.18, 0.2, 0.23, 0.25])\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}